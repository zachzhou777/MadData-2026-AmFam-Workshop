{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini + RAG: From Simple Answers to Validated, Structured Output\n",
    "\n",
    "This notebook is a continuation of `hugging_face_chromadb_demo.ipynb`. Make sure you have run that notebook first so that the ChromaDB collection is persisted to disk.\n",
    "\n",
    "**What you will learn:**\n",
    "1. **RAG with [Gemini](https://ai.google.dev/gemini-api/docs/quickstart)** — retrieve context, generate an answer\n",
    "2. **LLM-as-a-judge** — validate the answer against sources\n",
    "3. **Structured output** — JSON mode for predictable responses\n",
    "4. **Multimodal** — send PDF pages directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import json\n",
    "from google import genai\n",
    "import chromadb\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "PDF_DIR = DATA_DIR / 'pdf'\n",
    "TXT_DIR = DATA_DIR / 'txt'\n",
    "\n",
    "MODEL = 'gemini-2.5-flash'\n",
    "TEMPERATURE = 0\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize PDF-extracted text for reliable string matching.\n",
    "\n",
    "    Handles ligatures (fi→fi), hyphenated line breaks (frag-\\nmentation),\n",
    "    and extra whitespace from PDF layout.\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = re.sub(r'-\\s*\\n\\s*', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Initialize Gemini client; you'll probably be setting api_key instead\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=os.getenv('GCP_PROJECT'),\n",
    "    location=os.getenv('GCP_LOCATION')\n",
    ")\n",
    "\n",
    "# Load the persisted ChromaDB collection from previous notebook\n",
    "chroma_client = chromadb.PersistentClient()\n",
    "collection = chroma_client.get_collection(name='ostep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[threads-sema_1] Semaphores — page 2\n",
      "[threads-sema_2] Semaphores — page 3\n",
      "[threads-sema_17] Semaphores — page 18\n",
      "[threads-sema_0] Semaphores — page 1\n",
      "[threads-sema_4] Semaphores — page 5\n"
     ]
    }
   ],
   "source": [
    "# Ask a question and retrieve relevant documents\n",
    "question = \"What is a semaphore and how is it used?\"\n",
    "results = collection.query(query_texts=[question], n_results=5, include=['documents'])\n",
    "\n",
    "retrieved_docs = []\n",
    "for doc_id, doc_text in zip(results['ids'][0], results['documents'][0]):\n",
    "    chapter_id, page_idx_str = doc_id.rsplit('_', 1)\n",
    "    page_index = int(page_idx_str)\n",
    "    page_number = page_index + 1\n",
    "    first_page = (TXT_DIR / chapter_id / '0.txt').read_text()\n",
    "    chapter_name = first_page.split('\\n')[1].strip()\n",
    "    retrieved_docs.append({\n",
    "        'doc_id': doc_id,\n",
    "        'chapter_id': chapter_id,\n",
    "        'chapter_name': chapter_name,\n",
    "        'page_index': page_index,\n",
    "        'page_number': page_number,\n",
    "        'text': normalize_text(doc_text)\n",
    "    })\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"[{doc['doc_id']}] {doc['chapter_name']} — page {doc['page_number']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ask Gemini a Question\n",
    "\n",
    "We have 5 relevant pages from ChromaDB. Let's send them to Gemini as context and ask our question. The simplest approach: just get a free-text answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question using ONLY the provided source documents.\n",
      "\n",
      "Question:\n",
      "What is a semaphore and how is it used?\n",
      "\n",
      "Sources:\n",
      "<SOURCE chapter_id=\"threads-sema\" page_number=\"2\">\n",
      "2 SEMAPHORES the semaphore, we must first initialize it to some value, as the code in Figure 31.1 does. 1 #include <semaphore.h> 2 sem_t s; 3 sem_init(&s, 0, 1); Figure 31.1: Initializing A Semaphore In the figure, we declare a semaphore s and initialize it to the value 1 by passing 1 in as the third argument. The second argument to sem_init() will be set to 0 in all of the examples we’ll see; this indicates that the semaphore is shared between threads in the same process. See the man page for details on other usages of semaphores (namely, how they can be used to synchronize access across different processes), which require a different value for that second argument. After a semaphore is initialized, we can call one of two functions to interact with it, sem_wait() or sem_post(). The behavior of these two functions \n"
     ]
    }
   ],
   "source": [
    "context = \"\"\n",
    "for doc in retrieved_docs:\n",
    "    context += (\n",
    "        f\"<SOURCE chapter_id=\\\"{doc['chapter_id']}\\\" page_number=\\\"{doc['page_number']}\\\">\\n\"\n",
    "        f\"{doc['text']}\\n\"\n",
    "        f\"</SOURCE>\\n\\n\"\n",
    "    )\n",
    "\n",
    "prompt = f\"\"\"Answer the question using ONLY the provided source documents.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Sources:\n",
    "{context}\"\"\"\n",
    "\n",
    "print(prompt[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's check the AI response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A semaphore is an object with an integer value that can be manipulated using two routines: `sem_wait()` and `sem_post()`. The initial value of the semaphore determines its behavior.\n",
      "\n",
      "The routines operate as follows:\n",
      "*   `sem_wait(sem_t *s)`: Decrements the value of semaphore `s` by one. If the value of semaphore `s` becomes negative, the calling thread will wait (suspend execution). If multiple threads call `sem_wait()`, they can all be queued waiting to be woken.\n",
      "*   `sem_post(sem_t *s)`: Increments the value of semaphore `s` by one. If there are one or more threads waiting, it wakes one of them up.\n",
      "The value of the semaphore, when negative, is equal to the number of waiting threads.\n",
      "\n",
      "Semaphores are used as a synchronization primitive and can function as both locks and condition variables.\n",
      "\n",
      "Here are two primary ways semaphores are used:\n",
      "\n",
      "1.  **As a Lock (Binary Semaphore):**\n",
      "    *   A semaphore can be used to protect a critical section, ensuring that only one thread can execute it at \n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents=prompt,\n",
    "    config={'temperature': TEMPERATURE}\n",
    ")\n",
    "print(response.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like a solid answer. But how do we know it's actually correct and derived from our textbook sources? LLMs are prone to hallucination — they can produce fluent, confident text that's completely made up.\n",
    "\n",
    "There are several ways to address this. In this demo, we'll look at one approach: **LLM-as-a-judge**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM-as-a-Judge: Is the Answer Grounded?\n",
    "\n",
    "The idea is simple: use a second Gemini call whose only job is to check whether each claim in the answer is supported by the source text. The judge task is easier than generation — it's just comparing claims against documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us build the AI judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is how the AI judge prompt looks like:\n",
      "You are a fact-checker. Given an AI-generated answer and the source documents it was based on, determine if every claim in the answer is supported by the sources.\n",
      "\n",
      "Return JSON with this format:\n",
      "{\n",
      "    \"is_grounded\": true/false,\n",
      "    \"unsupported_claims\": [\n",
      "        {\"claim\": \"...\", \"explanation\": \"...\"}\n",
      "    ]\n",
      "}\n",
      "\n",
      "If all claims are supported, set is_grounded to true and unsupported_claims to an empty list.\n",
      "\n",
      "AI-generated answer:\n",
      "A semaphore is an object with an integer value that can be manipulated using two routines: `sem_wait()` and `sem_post()`. The initial value of the semaphore determines its behavior.\n",
      "\n",
      "The routines operate as follows:\n",
      "*   `sem_wait(sem_t *s)`: Decrements the value of semaphore `s` by one. If the value of semaphore `s` becomes negative, the calling thread will wait (suspend execution). If multiple threads call `sem_wait()`, they can all be queued waiting to be woken.\n",
      "*   `sem_post(sem_t *s)`: Increments the value of semaphore `s` by one. If there are one or more threads\n"
     ]
    }
   ],
   "source": [
    "judge_prompt = f\"\"\"You are a fact-checker. Given an AI-generated answer and the source documents it was based on, determine if every claim in the answer is supported by the sources.\n",
    "\n",
    "Return JSON with this format:\n",
    "{{\n",
    "    \"is_grounded\": true/false,\n",
    "    \"unsupported_claims\": [\n",
    "        {{\"claim\": \"...\", \"explanation\": \"...\"}}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "If all claims are supported, set is_grounded to true and unsupported_claims to an empty list.\n",
    "\n",
    "AI-generated answer:\n",
    "{response.text}\n",
    "\n",
    "Source documents:\n",
    "{context}\"\"\"\n",
    "\n",
    "print(\"Here is how the AI judge prompt looks like:\")\n",
    "print(judge_prompt[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the AI judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grounded: True\n",
      "All claims are supported by the sources.\n"
     ]
    }
   ],
   "source": [
    "judge_response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents=judge_prompt,\n",
    "    config={\n",
    "        'temperature': TEMPERATURE,\n",
    "        'response_mime_type': 'application/json'\n",
    "    }\n",
    ")\n",
    "\n",
    "verdict = json.loads(judge_response.text)\n",
    "print(f\"Grounded: {verdict['is_grounded']}\")\n",
    "if verdict['unsupported_claims']:\n",
    "    for claim in verdict['unsupported_claims']:\n",
    "        print(f\"  - {claim['claim']}: {claim['explanation']}\")\n",
    "else:\n",
    "    print(\"All claims are supported by the sources.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It Together: Retrieve → Generate → Judge → Retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG + LLM judge](rag_judge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's wrap the whole pipeline into a reusable function. The flow:\n",
    "\n",
    "1. **Retrieve** relevant pages from ChromaDB\n",
    "2. **Generate** a free-text answer with Gemini\n",
    "3. **Judge** — ask a second Gemini call if the answer is grounded\n",
    "4. **Retry** — if not grounded, feed back the unsupported claims and regenerate\n",
    "\n",
    "First, we'll extract the retrieval + context-building logic into a helper (it was duplicated in cells above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_build_context(question, collection, n_results=5):\n",
    "    \"\"\"Retrieve relevant docs from ChromaDB and format them as labeled context.\"\"\"\n",
    "    results = collection.query(query_texts=[question], n_results=n_results, include=['documents'])\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for doc_id, doc_text in zip(results['ids'][0], results['documents'][0]):\n",
    "        chapter_id, page_idx_str = doc_id.rsplit('_', 1)\n",
    "        page_index = int(page_idx_str)\n",
    "        page_number = page_index + 1\n",
    "        first_page = (TXT_DIR / chapter_id / '0.txt').read_text()\n",
    "        chapter_name = first_page.split('\\n')[1].strip()\n",
    "        retrieved_docs.append({\n",
    "            'doc_id': doc_id,\n",
    "            'chapter_id': chapter_id,\n",
    "            'chapter_name': chapter_name,\n",
    "            'page_index': page_index,\n",
    "            'page_number': page_number,\n",
    "            'text': normalize_text(doc_text)\n",
    "        })\n",
    "\n",
    "    context = \"\"\n",
    "    for doc in retrieved_docs:\n",
    "        context += (\n",
    "            f\"<SOURCE chapter_id=\\\"{doc['chapter_id']}\\\" page_number=\\\"{doc['page_number']}\\\">\\n\"\n",
    "            f\"{doc['text']}\\n\"\n",
    "            f\"</SOURCE>\\n\\n\"\n",
    "        )\n",
    "\n",
    "    return retrieved_docs, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the main function. It calls the helper above, then runs a **generate → judge → retry** loop:\n",
    "\n",
    "- **Generate**: ask Gemini to answer using only the source documents.\n",
    "- **Judge**: a second Gemini call checks if every claim is supported by the sources.\n",
    "- **Retry**: if the judge finds unsupported claims, we feed them back as error feedback and regenerate (up to `max_retries` times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_judge(question, collection, max_retries=3):\n",
    "    \"\"\"Full RAG pipeline: retrieve -> generate -> judge -> retry if not grounded.\"\"\"\n",
    "    retrieved_docs, context = retrieve_and_build_context(question, collection)\n",
    "\n",
    "    error_feedback = \"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        # Generate free-text answer\n",
    "        prompt = f\"\"\"Answer the question using ONLY the provided source documents.\n",
    "        {error_feedback}\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Sources:\n",
    "        {context}\"\"\"\n",
    "\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=prompt,\n",
    "            config={'temperature': TEMPERATURE}\n",
    "        )\n",
    "        answer_text = response.text\n",
    "\n",
    "        # Judge: is the answer grounded?\n",
    "        judge_prompt = f\"\"\"You are a fact-checker. Given an AI-generated answer and the source documents it was based on, determine if every claim in the answer is supported by the sources.\n",
    "\n",
    "        Return JSON with this format:\n",
    "        {{\n",
    "            \"is_grounded\": true/false,\n",
    "            \"unsupported_claims\": [\n",
    "                {{\"claim\": \"...\", \"explanation\": \"...\"}}\n",
    "            ]\n",
    "        }}\n",
    "\n",
    "        If all claims are supported, set is_grounded to true and unsupported_claims to an empty list.\n",
    "\n",
    "        AI-generated answer:\n",
    "        {answer_text}\n",
    "\n",
    "        Source documents:\n",
    "        {context}\"\"\"\n",
    "\n",
    "        judge_response = client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=judge_prompt,\n",
    "            config={\n",
    "                'temperature': TEMPERATURE,\n",
    "                'response_mime_type': 'application/json'\n",
    "            }\n",
    "        )\n",
    "        verdict = json.loads(judge_response.text)\n",
    "\n",
    "        if verdict['is_grounded']:\n",
    "            print(f'Attempt {attempt}: grounded! All claims supported.')\n",
    "            return answer_text, retrieved_docs\n",
    "\n",
    "        # Not grounded — retry with feedback\n",
    "        print(f'Attempt {attempt}: not grounded, retrying...')\n",
    "        for claim in verdict['unsupported_claims']:\n",
    "            print(f\"  - {claim['claim']}: {claim['explanation']}\")\n",
    "        error_feedback = (\n",
    "            \"\\nA fact-checker found these unsupported claims in your previous answer — fix them:\\n\"\n",
    "            + '\\n'.join(f\"- {c['claim']}: {c['explanation']}\" for c in verdict['unsupported_claims'])\n",
    "            + '\\n'\n",
    "        )\n",
    "\n",
    "    print(f'Failed after {max_retries} attempts.')\n",
    "    return answer_text, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: grounded! All claims supported.\n",
      "\n",
      "=== Answer ===\n",
      "A semaphore is an object with an integer value that can be manipulated using two routines: `sem_wait()` and `sem_post()`. Before use, a semaphore must be initialized to some value using `sem_init()`.\n",
      "\n",
      "**How it is used:**\n",
      "\n",
      "1.  **Initialization:** A semaphore `s` is initialized using `sem_init(&s, 0, initial_value)`. The second argument `0` indicates it's shared between threads in the same process.\n",
      "\n",
      "2.  **`sem_wait()`:**\n",
      "    *   Decrements the value of the semaphore by one.\n",
      "    *   If the semaphore's value becomes negative, the calling thread suspends execution and waits.\n",
      "    *   If the value is zero or positive, it returns immediately.\n",
      "    *   Multiple threads can call `sem_wait()` and be queued waiting to be woken.\n",
      "\n",
      "3.  **`sem_post()`:**\n",
      "    *   Increments the value of the semaphore by one.\n",
      "    *   If there are one or more threads waiting (i.e., the semaphore's value was negative before the increment), it wakes one of them up.\n",
      "\n",
      "**Specific Use Cases:**\n",
      "\n",
      "*   **As a Lock (Binary Semaphore\n"
     ]
    }
   ],
   "source": [
    "answer_text, _ = query_with_judge(question, collection)\n",
    "print()\n",
    "print('=== Answer ===')\n",
    "print(answer_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does LLM-as-a-judge work?\n",
    "\n",
    "- **Simpler task**: The judge only needs to answer \"is this claim supported?\" — a much easier job than writing the answer in the first place.\n",
    "- **Source text in context**: The judge has the source documents right in front of it for direct comparison.\n",
    "- **Catches semantic issues**: Unlike string matching, the judge can detect paraphrased hallucinations — claims that *sound* right but aren't actually in the sources.\n",
    "- **Tradeoff**: The judge isn't deterministic — it can make mistakes too. But for catching whether an *answer* is grounded (not just whether a *quote* was copied correctly), it's a better fit than string matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Structured Output\n",
    "\n",
    "So far, Gemini has returned free-text answers. That's fine for a chatbot, but real applications need predictable structure — a frontend needs specific fields, a database needs typed columns, a downstream API needs a fixed schema.\n",
    "\n",
    "Gemini's JSON mode (`response_mime_type: 'application/json'`) guarantees valid JSON output. We describe the shape we want in the prompt, and Gemini returns it — same technique we used for the judge, but now with a richer schema that includes citations and key concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Answer the question using ONLY the provided source documents.\n",
    "\n",
    "Return JSON with this format:\n",
    "{{\n",
    "    \"answer\": \"your answer here\",\n",
    "    \"citations\": [\n",
    "        {{\"chapter_id\": \"...\", \"page_number\": 1}}\n",
    "    ],\n",
    "    \"key_concepts\": [\"concept1\", \"concept2\"]\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- For each citation, set chapter_id and page_number to match the SOURCE tag exactly.\n",
    "- For key_concepts, list important terms that appear in the source text.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Sources:\n",
    "{context}\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents=prompt,\n",
    "    config={\n",
    "        'temperature': TEMPERATURE,\n",
    "        'response_mime_type': 'application/json'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"answer\": \"A semaphore is an object characterized by an integer value, which can be manipulated using two primary routines: `sem_wait()` and `sem_post()`. Before use, a semaphore must be initialized to a specific value using `sem_init()`. The `sem_wait()` routine decrements the semaphore's value and will cause the calling thread to suspend execution if the value becomes negative (or is not greater than or equal to 0). Conversely, `sem_post()` increments the semaphore's value and, if there are threads waiting, wakes one of them up.\\n\\nSemaphores are used for various synchronization purposes:\\n1.  **As a lock (binary semaphore)**: To protect critical sections, a semaphore is initialized to 1. A thread calls `sem_wait()` before entering the critical section and `sem_post()` upon exiting, ensuring mutual exclusion.\\n2.  **As an ordering primitive**: To make one thread wait for another to complete an action. For instance, a parent thread can wait for a child thread to finish by initializing the semaphore to 0. The parent calls `sem_wait()`, and the child calls `sem_post()` when its task is complete.\",\n",
      "    \"citations\": [\n",
      "        {\n",
      "            \"chapter_id\": \"threads-sema\",\n",
      "            \"page_number\": 1\n",
      "        },\n",
      "        {\n",
      "            \"chapter_id\": \"threads-sema\",\n",
      "            \"page_number\": 2\n",
      "        },\n",
      "        {\n",
      "            \"chapter_id\": \"threads-sema\",\n",
      "            \"page_number\": 3\n",
      "        },\n",
      "        {\n",
      "            \"chapter_id\": \"threads-sema\",\n",
      "            \"page_number\": 5\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Print as json\n",
    "print(response.text[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Answer ===\n",
      "A semaphore is an object characterized by an integer value, which can be manipulated using two primary routines: `sem_wait()` and `sem_post()`. Before use, a semaphore must be initialized to a specific value using `sem_init()`. The `sem_wait()` routine decrements the semaphore's value and will cause the calling thread to suspend execution if the value becomes negative (or is not greater than or equal to 0). Conversely, `sem_post()` increments the semaphore's value and, if there are threads waiting, wakes one of them up.\n",
      "\n",
      "Semaphores are used for various synchronization purposes:\n",
      "1.  **As a lock (binary semaphore)**: To protect critical sections, a semaphore is initialized to 1. A thread calls `sem_wait()` before entering the critical section and `sem_post()` upon exiting, ensuring mutual exclusion.\n",
      "2.  **As an ordering primitive**: To make one thread wait for another to complete an action. For instance, a parent thread can wait for a child thread to finish by initializing the semaphore \n",
      "\n",
      "=== Citations ===\n",
      "  [threads-sema p.1]\n",
      "  [threads-sema p.2]\n",
      "  [threads-sema p.3]\n",
      "  [threads-sema p.5]\n",
      "\n",
      "=== Key Concepts ===\n",
      "['semaphore', 'sem_wait', 'sem_post', 'sem_init', 'integer value', 'binary semaphore', 'lock', 'critical section', 'ordering primitive', 'synchronization']\n"
     ]
    }
   ],
   "source": [
    "# Print the response in user friendly format\n",
    "answer = json.loads(response.text)\n",
    "print('=== Answer ===')\n",
    "print(answer['answer'][:1000])\n",
    "print()\n",
    "print('=== Citations ===')\n",
    "for c in answer['citations']:\n",
    "    print(f\"  [{c['chapter_id']} p.{c['page_number']}]\")\n",
    "print()\n",
    "print('=== Key Concepts ===')\n",
    "print(answer['key_concepts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `response_mime_type: 'application/json'` and describing the desired shape in the prompt, Gemini returns valid JSON that we parse with `json.loads()`. Same technique as the judge call — just a richer schema with citations and key concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multimodal PDF Input (BONUS)\n",
    "\n",
    "So far, we've been sending extracted text to Gemini. But Gemini is multimodal — it can read PDFs directly. This is useful when the text extraction misses important information: tables, figures, diagrams, formulas, or scanned documents.\n",
    "\n",
    "Let's send the full PDF (as bytes) instead of plain text, and still get structured JSON back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending PDF: threads-sema.pdf (150277 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Use the top retrieved document from our original question\n",
    "top = retrieved_docs[0]\n",
    "pdf_path = PDF_DIR / f\"{top['chapter_id']}.pdf\"\n",
    "pdf_bytes = pdf_path.read_bytes()\n",
    "print(f\"Sending PDF: {pdf_path.name} ({len(pdf_bytes)} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Answer ===\n",
      "A semaphore is an object with an integer value that can be manipulated using two routines: `sem_wait()` and `sem_post()`. The `sem_wait()` routine decrements the semaphore's value by one and causes the calling thread to suspend execution if the value becomes negative (or indicates no resource is available). The `sem_post()` routine increments the semaphore's value by one and, if there are any threads waiting, wakes one of them up.\n",
      "\n",
      "Semaphores are versatile synchronization primitives used for various concurrency problems:\n",
      "1.  **As Locks (Binary Semaphores)**: By initializing a semaphore to 1, it can serve as a mutual exclusion lock. A thread calls `sem_wait()` before entering a critical section to acquire the lock and `sem_post()` after exiting to release it. If the lock is already held, other threads attempting to acquire it will wait.\n",
      "2.  **For Ordering Events**: By initializing a semaphore to 0, it can enforce a specific order of events between threads. One thread can wait (`sem_wait\n",
      "\n",
      "=== Citations ===\n",
      "  [threads-sema p.1]\n",
      "  [threads-sema p.2]\n",
      "  [threads-sema p.3]\n",
      "  [threads-sema p.4]\n",
      "  [threads-sema p.5]\n",
      "  [threads-sema p.7]\n",
      "  [threads-sema p.11]\n",
      "  [threads-sema p.13]\n",
      "  [threads-sema p.16]\n",
      "\n",
      "=== Key Concepts ===\n",
      "['Semaphore', 'sem_wait()', 'sem_post()', 'Synchronization Primitive', 'Locks', 'Binary Semaphore', 'Condition Variables', 'Critical Section', 'Ordering', 'Producer/Consumer Problem', 'Bounded Buffer', 'Reader-Writer Locks', 'Dining Philosophers Problem', 'Thread Throttling', 'Admission Control']\n"
     ]
    }
   ],
   "source": [
    "# Send the full PDF directly to Gemini\n",
    "multimodal_prompt = f\"\"\"You are given a PDF from a textbook on operating systems.\n",
    "Answer the question using ONLY what you see in this document.\n",
    "\n",
    "Return JSON with this format:\n",
    "{{\n",
    "    \"answer\": \"your answer here\",\n",
    "    \"citations\": [\n",
    "        {{\"chapter_id\": \"...\", \"page_number\": 1}}\n",
    "    ],\n",
    "    \"key_concepts\": [\"concept1\", \"concept2\"]\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Set chapter_id to \"{top['chapter_id']}\".\n",
    "- Set page_number to the page where you found the supporting information.\n",
    "- For key_concepts, list important terms that appear in the document.\n",
    "\n",
    "Question:\n",
    "{question}\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents=[\n",
    "        multimodal_prompt,\n",
    "        genai.types.Part.from_bytes(data=pdf_bytes, mime_type='application/pdf')\n",
    "    ],\n",
    "    config={\n",
    "        'temperature': TEMPERATURE,\n",
    "        'response_mime_type': 'application/json'\n",
    "    }\n",
    ")\n",
    "\n",
    "multimodal_answer = json.loads(response.text)\n",
    "print('=== Answer ===')\n",
    "print(multimodal_answer['answer'][:1000])\n",
    "print()\n",
    "print('=== Citations ===')\n",
    "for c in multimodal_answer['citations']:\n",
    "    print(f\"  [{c['chapter_id']} p.{c['page_number']}]\")\n",
    "print()\n",
    "print('=== Key Concepts ===')\n",
    "print(multimodal_answer['key_concepts'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When is multimodal input useful?\n",
    "\n",
    "- **Tables and figures**: text extraction often mangles table layouts; Gemini can read them directly from the PDF.\n",
    "- **Scanned documents**: if the PDF is a scan (image-based), text extraction may fail entirely.\n",
    "- **Formulas and diagrams**: mathematical notation and visual diagrams are better understood from the rendered page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Four patterns you can take away from this notebook:\n",
    "\n",
    "1. **RAG basics** — retrieve context from ChromaDB, send it to Gemini, get a free-text answer.\n",
    "\n",
    "2. **LLM-as-a-judge** — a second Gemini call validates whether the answer is grounded in the source text. If not, feed the unsupported claims back as feedback and retry.\n",
    "\n",
    "3. **Structured output** — use `response_mime_type: 'application/json'` to get valid JSON from Gemini. Describe the desired shape in the prompt and parse with `json.loads()`.\n",
    "\n",
    "4. **Multimodal input** — when text extraction isn't enough, send the full PDF directly to Gemini. You still get structured JSON back, and the same judge validates it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maddata2026 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
